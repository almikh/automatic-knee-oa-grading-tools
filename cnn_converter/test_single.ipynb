{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "import torch.utils.data\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "n_classes = 5\n",
    "dataset_dir = 'oai224/test/'\n",
    "\n",
    "# available architectures:\n",
    "# - densenet121, se_densenet121\n",
    "# - resnet18, resnet34, resnet50, se_resnet18, se_resnet34, se_resnet50, resnext50, se_resnext50\n",
    "# - inception_v3, se_inception_v3, xception, se_xception, inception_resnet\n",
    "cnn_type = 'se_xception'\n",
    "\n",
    "# available devices:\n",
    "# - cuda:0\n",
    "# - cpu\n",
    "torch_device = 'cpu'\n",
    "\n",
    "# ensemble\n",
    "cnn_model, frame_size, classes_header = utils.load_model(\n",
    "    cnn_type, \n",
    "    n_classes, \n",
    "    'models/se_xception.ckpt', \n",
    "    device = torch_device)\n",
    "\n",
    "# prepare model to cpu/gpu calc.\n",
    "if torch.cuda.is_available() and torch_device == 'cuda:0':\n",
    "    cnn_model.type(torch.cuda.FloatTensor)\n",
    "else:\n",
    "    cnn_model.type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "transforms_to_test = transforms.Compose([\n",
    "              transforms.Resize(frame_size), \n",
    "              transforms.ToTensor(),\n",
    "              transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(dataset_dir, transform=transforms_to_test)\n",
    "test_idx = list(range(len(test_dataset.imgs)))\n",
    "test_sampler = SequentialSampler(test_idx)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, sampler=test_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "total_params = sum(p.numel() for p in cnn_model.parameters())\n",
    "total_trainable_params = sum(p.numel() for p in cnn_model.parameters() if p.requires_grad)\n",
    "print(\"parameters number: \", total_params)\n",
    "print(\"trainable parameters number: \", total_trainable_params)\n",
    "\n",
    "predictions = []\n",
    "raw_predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  print(\"compute accuracy...\")\n",
    "  test_accuracy, predictions, ground_truth, raw_predictions = utils.compute_accuracy(cnn_model, test_loader)\n",
    "  print(\"test accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def performance_matrix(true,pred):\n",
    "    precision = metrics.precision_score(true,pred,average='weighted')\n",
    "    recall = metrics.recall_score(true,pred,average='weighted') # average='weighted'\n",
    "    accuracy = metrics.accuracy_score(true,pred)\n",
    "    f1_score = metrics.f1_score(true,pred,average='weighted')\n",
    "    print('Mean \\n  precision: {} \\n  recall: {}, \\n  accuracy: {}, \\n  f1_score: {}'.format(precision*100,recall*100,accuracy*100,f1_score*100))\n",
    "\n",
    "performance_matrix(ground_truth, predictions)\n",
    "\n",
    "# \n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(ground_truth, predictions)\n",
    "\n",
    "print('\\n')\n",
    "print('rows is precision, recall, fscore and support:')\n",
    "print(tabulate([precision, recall, fscore, support], headers=['0' , '1' , '2' , '3', '4'], tablefmt='orgtbl'))\n",
    "\n",
    "print('\\n')\n",
    "print('per-class accuracy:')\n",
    "cm = confusion_matrix(ground_truth, predictions)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(tabulate([cm.diagonal()], headers=['0' , '1' , '2' , '3', '4'], tablefmt='orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "            \n",
    "    # Compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "plot_confusion_matrix(ground_truth, predictions, classes=[\"0\", \"1\", \"2\", \"3\", \"4\"],title='Confusion matrix, without normalization')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "y_score = np.array(raw_predictions)\n",
    "y_test = ground_truth\n",
    "\n",
    "# Binarize the output\n",
    "if n_classes == 2:\n",
    "    _classes= [0, 1]\n",
    "    colors =  ['blue', 'red']\n",
    "    y_test = label_binarize(y_test, range(n_classes+1))[:,:-1]\n",
    "else:\n",
    "    _classes= [0, 1, 2, 3, 4]\n",
    "    colors = ['purple', 'green', 'blue', 'red', 'black']\n",
    "    y_test = label_binarize(y_test, classes=_classes,neg_label=0, pos_label=1, sparse_output=False)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [9, 5]\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve for grade {0} (AUC = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for multi-class data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
